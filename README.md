# ğŸ¤– AI Assistant with RAG and Streamlit

This project implements an **AI Assistant powered by Retrieval-Augmented Generation (RAG)**.  
It processes .docx, .txt, .pdf documents, builds a hybrid retriever using **Chroma (vector search)** and **BM25 (keyword search)**, and provides a **Streamlit-based chat interface**.  
Answers are generated by a Language Model (LLM) with reranking to improve accuracy.

---

## ğŸ¥ Demo Video

ğŸ‘‰ [Click here to watch the demo on YouTube](https://www.youtube.com/watch?v=NHYiGuLC9Bc)

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ .env                          # API keys (should be .gitignored)
â”œâ”€â”€ data/                         # Raw .docx, .pdf, .txt documents
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ venv/                         # Virtual environment (should be .gitignored)
â””â”€â”€ src/
    â”œâ”€â”€ chroma_db/                # Vector DB persisted by Chroma
    â”œâ”€â”€ agent.py                  # RAG chain using cloud LLM (OpenAI/Grok)
    â”œâ”€â”€ app.py                    # Streamlit main interface
    â”œâ”€â”€ local.py                  # RAG chain using local Ollama model
    â”œâ”€â”€ process_data.py           # Document processing & summarization
    â””â”€â”€ retriever_and_vectordb.py # Hybrid retriever: Chroma + BM25
```

---

## ğŸŒŸ Features

- **Document Processing**: .docx, .txt, .pdf â†’ chunked & summarized with LLM  
- **Hybrid Retrieval**: 70% Chroma (vector) + 30% BM25 (keyword) retrieval  
- **Flexible LLM Support**: GPT-4 (OpenAI), Grok (xAI), or local Ollama  
- **Reranking**: Uses FlagReranker for accuracy  
- **Web UI**: Streamlit app with chat interface and reranker score display  
- **Storage**: Saves chunks in processed_docs and vectors in chroma_db/  

---

## ğŸ§° Prerequisites

### Software
- Python 3.8+
- Ollama (if using local model)

### Install dependencies:

```bash
pip install -r requirements.txt
```

### requirements.txt should include:

```
streamlit
langchain
langchain-openai
langchain-community
langchain-ollama
langchain-huggingface
python-docx
pypdf
pymupdf
sentence-transformers
FlagEmbedding
python-dotenv
tenacity
```

---

## ğŸ“ Documents Directory

Put .docx, .txt, or .pdf files in a folder like:
```
./data/
```

Default is: `./data`  
You can change it in `process_data.py`.

---

## ğŸš€ How to Use

### Step 1: Prepare data

```bash
python process_data.py
```

### Step 2: Run the app

```bash
streamlit run app.py
```

Then go to [http://localhost:8501](http://localhost:8501)

### Step 3: In the sidebar
- Select **Embedding Model**: all-MiniLM-L12-v2
- Select **AI Model**: GPT-4, Grok, or Ollama

If using Ollama (local), make sure to:

```bash
ollama pull llama3
```

And update `local.py` with the model name.

---

## ğŸ’¬ Example Workflow

1. Files in data folder:
    - `mamba.pdf`
    - `notes.txt`
    - `plan.docx`

2. Run app:

```bash
streamlit run app.py
```

3. Enter a query that corresponds to the content of the document you want to retrieve:

```
e.g :What is Mamba?
```

4. The assistant:
    - Retrieves chunks
    - Reranks using FlagReranker
    - Generates response from top documents
    - Shows scores (e.g. 92.5%, 89.3%)

---

## ğŸ›  Components Explained

### process_data.py
- Splits documents into 5000-char chunks (with 500/1000 overlap)
- Adds LLM-based summaries
- Saves to `processed_docs.txt`

> âš ï¸ If `meta-llama/llama-4-scout...` is unsupported, use:  
> `llama3-8b-8192` or `grok` depending on your provider

---

### retriever_and_vectordb.py
- Builds `EnsembleRetriever` (Chroma + BM25)
- Adds UUIDs and uses HuggingFaceEmbeddings

---

### agent.py
- Loads cloud LLM (GPT-4 or Grok)
- Creates RAG chain with retriever and prompt

---

### local.py
- Uses Ollama local LLM
- Set model name (e.g. `llama3`)

---

### app.py
- Runs Streamlit UI
- Supports reranker + model switching

---

## ğŸ§ª Outputs

- âœ… Processed docs â†’ `processed_docs`
- âœ… Vector DB â†’ `chroma_db/`
- âœ… First 2 results printed in console
- âœ… Reranker scores displayed in UI

---

## ğŸ Troubleshooting

- **API errors**: Check `.env` and keys  
- **Model loading failed**: Change model in `process_data.py` / `agent.py`  
- **Ollama not running**: Start it and `ollama pull <model>`  
- **PDF not loading**: Use `fitz` instead of `PyPDFLoader`  

---

## ğŸ’¡ Future Improvements

- Support more LLMs  
- Better PDF parsing with `fitz`  
- Include metadata (filename, page #)  
- Add caching to avoid repeated LLM calls  
- Allow custom prompts in UI  

---

## ğŸ¤ Contact

Nguyen Tien Anh  
ğŸ“§ anhnguyentien8365@gmail.com
