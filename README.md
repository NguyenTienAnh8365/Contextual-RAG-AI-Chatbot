# 🤖 AI Assistant with RAG and Streamlit

This project implements **AI Assistant powered by Contextual Retrieval-Augmented Generation**.  
It processes .docx, .txt, .pdf documents, builds a hybrid retriever using **Chroma (vector search)** and **BM25 (keyword search)**, and provides a **Streamlit-based chat interface**.  
Answers are generated by a Language Model (LLM) with **reranking** to improve accuracy.

---

## 🎥 Demo Video

👉 [Click here to watch the demo on YouTube](https://www.youtube.com/watch?v=NHYiGuLC9Bc)

---

## 📂 Project Structure

```
.
├── .gitignore                    # Git ignore rules
├── .env                          # API keys
├── data/                         # Raw .docx, .pdf, .txt documents
├── requirements.txt              # Python dependencies
├── README.md                     # Project documentation
├── venv/                         # Virtual environment
└── src/
    ├── chroma_db/                # Vector DB persisted by Chroma
    ├── agent.py                  # RAG chain using cloud LLM (OpenAI/Grok)
    ├── app.py                    # Streamlit main interface
    ├── local.py                  # RAG chain using local Ollama model
    ├── process_data.py           # Document processing & generate context for each chunk by LLM
    └── retriever_and_vectordb.py # Hybrid retriever: Chroma + BM25
```

---

## 🌟 Features

- **Document Processing**: .docx, .txt, .pdf → chunked & generated context with LLM  
- **Hybrid Retrieval**: 70% Chroma (vector) + 30% BM25 (keyword) retrieval  
- **Flexible LLM Support**: GPT-4 (OpenAI), Grok (xAI), or local Ollama  
- **Reranking**: Uses FlagReranker for accuracy  
- **Web UI**: Streamlit app with chat interface and reranker score display  
- **Storage**: Saves chunks in processed_docs and vectors in chroma_db/  

---

## 🧰 Prerequisites

### Software
- Python 3.8+
- Ollama (if using local model)

### Install dependencies:

```bash
pip install -r requirements.txt
```

### requirements.txt should include:

```
streamlit
langchain
langchain-openai
langchain-community
langchain-ollama
langchain-huggingface
python-docx
pypdf
pymupdf
sentence-transformers
FlagEmbedding
python-dotenv
tenacity
```

---

## 📁 Documents Directory

Put .docx, .txt, or .pdf files in a folder like:
```
./data/
```

Default is: `./data`  
You can change it in `process_data.py`.

---

## 🚀 How to Use

### Step 1: Prepare data

```bash
python process_data.py
```

### Step 2: Run the app

```bash
streamlit run app.py
```

Then go to [http://localhost:8501](http://localhost:8501)

### Step 3: In the sidebar
- Select **Embedding Model**: all-MiniLM-L12-v2
- Select **AI Model**: GPT-4, Grok, or Ollama

If using Ollama (local), make sure to:

```bash
ollama pull llama3
```

And update `local.py` with the model name.

---

## 💬 Example Workflow

1. Files in data folder:
    - `file.pdf`
    - `notes.txt`
    - `plan.docx`

2. Run app:

```bash
streamlit run app.py
```

3. Enter a query that corresponds to the content of the document you want to retrieve:

```
e.g :What is Mamba?
```

4. The assistant:
    - Retrieves chunks
    - Reranks using FlagReranker
    - Generates response from top documents
    - Shows scores (e.g. 92.5%, 89.3%)

---

## 🛠 Components Explained

### process_data.py
- Splits documents into 5000-char chunks (with 500/1000 overlap)
- Call the large language model to create context for each block
- Saves to `processed_docs`

---

### retriever_and_vectordb.py
- Builds `EnsembleRetriever` (Chroma vector search + BM25)
- Adds UUIDs and uses HuggingFaceEmbeddings

---

### agent.py
- Loads cloud LLM (GPT-4 or Grok)
- Creates RAG chain with retriever and prompt

---

### local.py
- Uses Ollama local LLM
- Set model name (e.g. `llama3`)

---

### app.py
- Runs Streamlit UI
- Supports reranker + model switching

---

## 🧪 Outputs

- ✅ Processed docs → `processed_docs`
- ✅ Vector DB → `chroma_db/`
- ✅ First 2 results printed in console
- ✅ Respond to user requests with the correct content in the document
- ✅ Reranker scores displayed in UI

---

## 🐞 Troubleshooting

- **API errors**: Check `.env` and keys  
- **Model loading failed**: Make sure your API key is working correctly in `process_data.py` / `agent.py`  
- **Ollama not running**: Start it and `ollama pull <model>`  
- **PDF not loading**: Use `fitz` instead of `PyPDFLoader`  

---

## 💡 Future Improvements

- Support more LLMs  
- Better PDF parsing with `fitz`  
- Include metadata (filename, page #)  
- Add caching to avoid repeated LLM calls  
- Allow custom prompts in UI  

---

## 🤝 Contact

Nguyen Tien Anh  
📧 anhnguyentien8365@gmail.com
