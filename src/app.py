# === IMPORT CÃC THÆ¯ VIá»†N Cáº¦N THIáº¾T ===
import streamlit as st
from dotenv import load_dotenv
from agent import get_llm_and_agent
from local import get_llm_local as get_ollama_local
from retriever_and_vectordb import retriever
import os
import logging
import time
from FlagEmbedding import FlagReranker

# === Táº¢I CÃC BIáº¾N MÃ”I TRÆ¯á»œNG ===
load_dotenv()

# === Láº¤Y API KEY á»ª BIáº¾N MÃ”I TRÆ¯á»œNG ===
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found in environment variables")

XAI_API_KEY = os.getenv("XAI_API_KEY")
if not XAI_API_KEY:
    raise ValueError("XAI_API_KEY not found in environment variables")

# === Cáº¤U HÃŒNH TRANG WEB ===
def setup_page():
    """
    Cáº¥u hÃ¬nh cÆ¡ báº£n cho trang web.
    """
    st.set_page_config(page_title="AI Assistant", page_icon="ğŸ’¬", layout="wide")

# === KHá»I Táº O á»¨NG Dá»¤NG ===
def initialize_app():
    """
    Khá»Ÿi táº¡o cÃ¡c cÃ i Ä‘áº·t cáº§n thiáº¿t.
    """
    load_dotenv()
    setup_page()

# === THANH CÃ”NG Cá»¤ BÃŠN TRÃI ===
def setup_sidebar():
    """
    Táº¡o giao diá»‡n thanh bÃªn trÃ¡i.
    """
    with st.sidebar:
        st.title("âš™ï¸ Cáº¥u hÃ¬nh")
        st.markdown("---")

        # Chá»n embeddings model
        embeddings_choice = st.radio("ğŸ”¤ Chá»n Embedding Model:", ["Sentence-Transformer"])

        # Chá»n AI Model
        model_choice = st.radio("ğŸ¤– Chá»n AI Model:", ["GPT-4", "Grok", "Ollama (Local)"])

        return embeddings_choice, model_choice

# === GIAO DIá»†N CHAT ===
def setup_chat_interface(model_choice):
    """
    Cáº¥u hÃ¬nh giao diá»‡n chat.
    """
    st.title("ğŸ’¬ AI Assistant")
    st.caption({
        "GPT-4": "ğŸš€ Há»— trá»£ bá»Ÿi GPT-4",
        "Grok": "ğŸš€ Há»— trá»£ bá»Ÿi X.AI Grok",
        "Ollama (Local)": "ğŸš€ Há»— trá»£ bá»Ÿi Ollama"
    }.get(model_choice, "ğŸš€ Trá»£ lÃ½ AI cá»§a báº¡n"))

    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n?"}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).markdown(msg["content"])

# Táº¡o reranker tá»« mÃ´ hÃ¬nh FlagReranker
reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)

# === Xá»¬ LÃ Äáº¦U VÃ€O NGÆ¯á»œI DÃ™NG ===
def handle_user_input_with_reranking(agent_executor, retriever):
    """
    Xá»­ lÃ½ Ä‘áº§u vÃ o tá»« ngÆ°á»i dÃ¹ng vÃ  hiá»ƒn thá»‹ cÃ¢u tráº£ lá»i vá»›i reranking.
    """
    if prompt := st.chat_input("Há»i tÃ´i Ä‘iá»u gÃ¬ báº¡n mong muá»‘n!"):
        st.session_state.messages.append({"role": "human", "content": prompt})
        st.chat_message("human").markdown(prompt)

        # Cáº­p nháº­t hiá»‡u á»©ng cho tráº¡ng thÃ¡i suy nghÄ©
        response_placeholder = st.chat_message("assistant").empty()
        thinking_texts = ["ğŸ’­ Äang suy nghÄ©", "ğŸ’­ Äang suy nghÄ©.", "ğŸ’­ Äang suy nghÄ©..", "ğŸ’­ Äang suy nghÄ©..."]

        # Táº¡o hiá»‡u á»©ng "..." chuyá»ƒn Ä‘á»™ng
        for _ in range(25):
            for text in thinking_texts:
                response_placeholder.markdown(text)  # Cáº­p nháº­t tráº¡ng thÃ¡i
                time.sleep(0.25)  # Ä‘á»™ trá»… giá»¯a cÃ¡c láº§n thay Ä‘á»•i

        try:
            # Truyá»n cÃ¢u há»i vÃ o agent executor vÃ  láº¥y cÃ¢u tráº£ lá»i tá»« LLM
            USER_QUESTION = prompt
            
            # Láº¥y cÃ¡c tÃ i liá»‡u liÃªn quan tá»« retriever
            documents = retriever.get_relevant_documents(USER_QUESTION)

            # Chá»n tÃ i liá»‡u cÃ³ Ä‘iá»ƒm rerank cao nháº¥t
            reranker_scores = reranker.compute_score([[USER_QUESTION, doc.page_content] for doc in documents], normalize=True)

            # Chuyá»ƒn Ä‘á»•i Ä‘iá»ƒm thÃ nh tá»· lá»‡ pháº§n trÄƒm (0-100%)
            reranker_percentage_scores = [score * 100 for score in reranker_scores]

            # In ra Ä‘iá»ƒm sá»‘ dÆ°á»›i dáº¡ng tá»· lá»‡ pháº§n trÄƒm
            for i, score in enumerate(reranker_percentage_scores):
                st.markdown(f"Äiá»ƒm sá»‘ giá»¯a cÃ¢u há»i vÃ  tÃ i liá»‡u {i + 1}: {score:.2f}%")

            # Chá»n tÃ i liá»‡u cÃ³ Ä‘iá»ƒm rerank cao nháº¥t Ä‘á»ƒ dá»±a vÃ o Ä‘Ã³ táº¡o cÃ¢u tráº£ lá»i tá»« LLM
            best_answer_index = reranker_scores.index(max(reranker_scores))
            best_document = documents[best_answer_index].page_content  # Chá»n tÃ i liá»‡u cÃ³ Ä‘iá»ƒm cao nháº¥t

            # Sá»­ dá»¥ng tÃ i liá»‡u cÃ³ Ä‘iá»ƒm cao nháº¥t Ä‘á»ƒ táº¡o cÃ¢u tráº£ lá»i tá»« LLM
            # Truyá»n cÃ¢u há»i vÃ  tÃ i liá»‡u vÃ o LLM Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i phÃ¹ há»£p
            if hasattr(agent_executor, 'invoke'):
                output_stream = agent_executor.invoke(f"{USER_QUESTION}", stream=True)
            else:
                raise ValueError("Agent Executor khÃ´ng cÃ³ phÆ°Æ¡ng thá»©c 'invoke'.")

            # CÃ¢u tráº£ lá»i cuá»‘i
            final_answer = ""
            for part in output_stream:
                final_answer += part  # Tráº£ lá»i tá»«ng pháº§n trong stream
                response_placeholder.markdown(f"ğŸ—£ï¸ CÃ¢u tráº£ lá»i: {final_answer}") # Cáº­p nháº­t cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng
                time.sleep(0.01)  # thá»i gian trá»… giá»¯a cÃ¡c tá»«

            # LÆ°u cÃ¢u tráº£ lá»i vÃ o session
            st.session_state.messages.append({"role": "assistant", "content": final_answer})

            # TÃ­nh toÃ¡n láº¡i Ä‘iá»ƒm rerank giá»¯a cÃ¢u tráº£ lá»i tá»« LLM vÃ  tÃ i cu há»i tá»« ngÆ°á»i dÃ¹ng
            final_reranker_score = reranker.compute_score([[USER_QUESTION, final_answer], [USER_QUESTION, best_document]], normalize=True)

            # In ra Ä‘iá»ƒm sá»‘ (káº¿t há»£p)
            st.markdown(f"Äiá»ƒm sá»‘ cho cÃ¢u tráº£ lá»i tá»« LLM: {((final_reranker_score[0] * 100 * 0.8 + final_reranker_score[1] * 100 * 0.2)):.2f}%")

        except Exception as e:
            response_placeholder.markdown(f"âŒ Lá»—i: {e}")
            st.error(f"Lá»—i trong khi xá»­ lÃ½: {e}")

# === CHáº Y á»¨NG Dá»¤NG ===
def main():

    initialize_app()

    embeddings_choice, model_choice = setup_sidebar()

    try:
        # Táº¡o agent cho mÃ´ hÃ¬nh Ä‘Ã£ chá»n
        if model_choice == "Ollama (Local)":
            agent_executor = get_ollama_local(retriever)  # Táº¡o agent cho Ollama local
        elif model_choice == "Grok":
            agent_executor = get_llm_and_agent(retriever, model_choice="grok")  # Táº¡o agent cho Grok
        elif model_choice == "GPT-4":
            agent_executor = get_llm_and_agent(retriever, model_choice="gpt4")  # Táº¡o agent cho GPT-4
        else:
            raise ValueError(f"MÃ´ hÃ¬nh '{model_choice}' khÃ´ng Ä‘Æ°á»£c há»— trá»£.")

        # Cáº¥u hÃ¬nh giao diá»‡n chat
        setup_chat_interface(model_choice)

        # Xá»­ lÃ½ Ä‘áº§u vÃ o cá»§a ngÆ°á»i dÃ¹ng vá»›i streaming
        handle_user_input_with_reranking(agent_executor, retriever)

    except Exception as e:
        logging.error(f"Lá»—i khi khá»Ÿi cháº¡y á»©ng dá»¥ng: {e}")
        st.error(f"ÄÃ£ xáº£y ra lá»—i: {e}")

if __name__ == "__main__":
    main()
